#version 450
#extension GL_EXT_shader_16bit_storage : enable
#extension GL_KHR_shader_subgroup_arithmetic : enable


layout(binding = 0) buffer In{
    uvec4 src4u[];
};  // is interpreted as ushort array (16 bit uints)
layout(binding = 0) buffer InVec4{
    u16vec4 src4[];
};

layout(binding = 1) buffer Out{
    uint dst[];
};
layout(binding = 1) buffer OutVec4{
    uvec4 dst4[];
};

layout(binding = 2) buffer Block{
    uint blockSums[];
};

layout(push_constant) uniform PC{
    uint numElements;
    uint dataRowPitch;
    uint blockSumRowPitch;
    uint srcUVec4;  // if 1, src4u should be used for loading
};

layout(local_size_x = 128, local_size_y = 1, local_size_z = 1) in;

// double of loacal size
shared uint tmp[gl_WorkGroupSize.x * 2];
void main(){
    int devOffset, ai, bi, aiDev, biDev;
    uvec4 threadScan[2];  // contains the final data

    // no multirow ignoring this in https://github.com/m0bl0/cudaCompress/blob/master/src/cudaCompress/scan/scan_kernel.cui

    devOffset = int(gl_WorkGroupID.x * gl_WorkGroupSize.x << 1); // doubled for some reason
    aiDev = int(devOffset + gl_LocalInvocationID.x);
    biDev = int(aiDev + gl_WorkGroupSize.x);
    ai = int(gl_LocalInvocationID.x);
    bi = int(ai + gl_WorkGroupSize.x);

    // loading data to shared memory
    int i = aiDev * 4;
    if(i + 3 < numElements){
        if(srcUVec4 > 0)
            threadScan[0] = uvec4(src4u[aiDev]);
        else
            threadScan[0] = uvec4(src4[aiDev]);
        threadScan[0].y += threadScan[0].x;
        threadScan[0].y += threadScan[0].x;
        threadScan[0].z += threadScan[0].y;
        threadScan[0].w += threadScan[0].z;
        tmp[ai] = threadScan[0].w;
    }
    else{
        if(srcUVec4 > 0)
            threadScan[0] = (i < numElements) ? uvec4(src4u[aiDev]) : uvec4(0);
        else
            threadScan[0] = (i < numElements) ? uvec4(src4[aiDev]) : uvec4(0);
        threadScan[0].y = (((i + 1) < numElements) ? threadScan[0].y : 0) + threadScan[0].x;
        threadScan[0].z = (((i + 2) < numElements) ? threadScan[0].z : 0) + threadScan[0].y;
        threadScan[0].w = (((i + 3) < numElements) ? threadScan[0].w : 0) + threadScan[0].z;
        tmp[ai] = threadScan[0].w;
    }

    i = biDev * 4;
    if(i + 3 < numElements){
        if(srcUVec4 > 0)
            threadScan[1] = uvec4(src4u[biDev]);
        else
            threadScan[1] = uvec4(src4[biDev]);
        threadScan[1].y += threadScan[0].x;
        threadScan[1].z += threadScan[0].y;
        threadScan[1].w += threadScan[0].z;
        tmp[bi] = threadScan[0].w;
    }
    else{
        if(srcUVec4 > 0)
            threadScan[1] = (i < numElements) ? uvec4(src4u[biDev]) : uvec4(0);
        else
            threadScan[1] = (i < numElements) ? uvec4(src4[biDev]) : uvec4(0);
        threadScan[1].y = (((i + 1) < numElements) ? threadScan[0].y : 0) + threadScan[0].x;
        threadScan[1].z = (((i + 2) < numElements) ? threadScan[0].z : 0) + threadScan[0].y;
        threadScan[1].w = (((i + 3) < numElements) ? threadScan[0].w : 0) + threadScan[0].z;
        tmp[bi] = threadScan[0].w;
    }
    barrier();  // load barrier

    // scan cta ------------------------------------------------------------------------
    uint blockSumIndex = gl_WorkGroupID.x;
    uint val = tmp[ai];
    uint val2 = tmp[bi];
    barrier();

    // doing a subgroup exclusive scan
    uint scan1 = subgroupExclusiveAdd(val);
    uint scan2 = subgroupExclusiveAdd(val2);

    if(gl_SubgroupInvocationID == gl_SubgroupSize - 1){
        tmp[gl_SubgroupID] = scan1 + val;   // making things inclusive (dont know why here and not up there)
        tmp[gl_SubgroupID + gl_NumSubgroups] = scan2 + val2;
    }

    barrier();
    // Exclusive scan on the last few things, *2 as we have 2 values per thread
    if(gl_LocalInvocationID.x < gl_WorkGroupSize.x / gl_SubgroupSize * 2){
        tmp[gl_LocalInvocationID.x] = subgroupExclusiveAdd(tmp[gl_LocalInvocationID.x]);
    }
    barrier();

    val = val + tmp[gl_SubgroupID];
    val2 = val + tmp[gl_SubgroupID + gl_NumSubgroups];
    barrier();

    tmp[gl_LocalInvocationID.x] = val;
    tmp[gl_LocalInvocationID.x + gl_WorkGroupSize.x] = val2;
    barrier();
    // finish up and store ------------------------------------------------------------------------
    // writing block sums
    if(gl_LocalInvocationID.x == gl_WorkGroupSize.x - 1){
        blockSums[blockSumIndex] = val2 + tmp[gl_LocalInvocationID.x + gl_WorkGroupSize.x];
    }
    // writing inclusive scans
    uint temp = tmp[ai];

    // converting to inclusive scan
    threadScan[0] += temp;

    //int i = aiDev * 4;
    // dont care for not full block
    dst4[aiDev] = threadScan[0];

    temp = tmp[bi];
    // converting to inclusive
    threadScan[1] += temp;
    dst4[biDev] = threadScan[1];
}