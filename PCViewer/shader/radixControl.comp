#version 460

#extension GL_GOOGLE_include_directive : enable
#extension GL_KHR_shader_subgroup_arithmetic : enable
#extension GL_KHR_shader_subgroup_ballot : enable

#include "radixHeader.glsl"

layout(local_size_x_id = 0, local_size_y = 1, local_size_z = 1) in;

// one controlshader invocation is done per global histogram
// ctl sizes in the dispatch info 
void main(){
    uint front = uniformInfo[0].pass & 1;   //make sure to have the correct pass number by using the back uniformInfo
    uint back = front ^ 1;
    uint curStart = uniformInfo[front].globalHistograms[gl_WorkGroupID.x].startOffset;
    uint curEnd = uniformInfo[front].globalHistograms[gl_WorkGroupID.x].endOffset;

    const uint binsPerThread = NUMKEYS / SUBGROUP_SIZE;
    uint localBins[binsPerThread];
    uint binsSizes[binsPerThread];
    uint binsGroups[binsPerThread];
    uint newBinCount = 0;
    uint amtGroups = 0; //sum of the needed work groups for the histogram calculation
    for(int i = 0; i < binsPerThread; ++i){
        localBins[i] = uniformInfo[front].globalHistograms[gl_WorkGroupID.x].keyCount[gl_LocalInvocationID.x * binsPerThread + i];
        uint curBinSize = localBins[i];
        if(gl_LocalInvocationID.x * binsPerThread + i != 0)
            curBinSize -= uniformInfo[front].globalHistograms[gl_WorkGroupID.x].keyCount[gl_LocalInvocationID.x * binsPerThread + i - 1];
        if(curBinSize != 0)
            ++newBinCount;
        binsSizes[i] = curBinSize;
        binsGroups[i] = (curBinSize + KPB - 1) / KPB;
        amtGroups += binsGroups[i];
    }
    //subgroup reduction for absolute new bin count + exclusive add to have the correct offsets
    uint bucketHistogramOffset = subgroupExclusiveAdd(newBinCount);
    uint groupsOffset = subgroupExclusiveAdd(amtGroups);
    newBinCount = subgroupAdd(newBinCount);
    amtGroups = subgroupAdd(amtGroups);
    //reserving memroy for new histograms and new subgroups
    uint histIndex = 0;
    uint groupIndex = 0;
    if(gl_LocalInvocationID.x == 0){
        histIndex = atomicAdd(uniformInfo[back].amtOfGlobalHistograms, newBinCount);
        groupIndex = atomicAdd(uniformInfo[back].amtOfBlocks, amtGroups);
    }
    histIndex = subgroupBroadcastFirst(histIndex);
    groupIndex = subgroupBroadcastFirst(groupIndex);
    uint binHistOffset = 0;
    uint blockOffset = groupIndex + groupsOffset;
    for(int i = 0; i < binsPerThread; ++i){
        if(binsSizes[i] != 0){
            uint curHistIndex = histIndex + bucketHistogramOffset + binHistOffset;
            uint curStartOffset = curStart + localBins[i] - binsSizes[i];
            uniformInfo[back].globalHistograms[curHistIndex].startOffset = curStartOffset;
            uniformInfo[back].globalHistograms[curHistIndex].endOffset = curStart + localBins[i];
            //maybe nulling not needed or can be done more efficiently
            for(uint nk = 0; nk < NUMKEYS; ++nk){
                uniformInfo[back].globalHistograms[curHistIndex].keyCount[nk] = 0;
            }
            for(uint g = 0; g < binsGroups[i]; ++g){
                groupInfos.i[blockOffset].globalHistIndex = curHistIndex;
                groupInfos.i[blockOffset].startOffset = curStartOffset + g * KPB;
                // no nulling needed, is set via assignment
                ++blockOffset;
            }
            ++binHistOffset;
        }
    }
}