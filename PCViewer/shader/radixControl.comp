#version 460

#extension GL_GOOGLE_include_directive : enable
#extension GL_KHR_shader_subgroup_arithmetic : enable
#extension GL_KHR_shader_subgroup_ballot : enable

#include "radixHeader.glsl"

layout(local_size_x_id = 0, local_size_y = 1, local_size_z = 1) in;
const uint binsPerThread = NUMKEYS / SUBGROUP_SIZE;

// one controlshader invocation is done per global histogram
// ctl sizes in the dispatch info 
shared uint localSortAmt;
shared uint localSortOffset;
shared LocalSort sharedLocalSorts[NUMKEYS];
void main(){
    uint front = uniformInfo[0].pass & 1;   //make sure to have the correct pass number by using the back uniformInfo
    uint back = front ^ 1;
    uint curStart = uniformInfo[front].globalHistograms[gl_WorkGroupID.x].startOffset;
    uint curEnd = uniformInfo[front].globalHistograms[gl_WorkGroupID.x].endOffset;

    uint localBins[binsPerThread];
    uint binsSizes[binsPerThread];
    uint binsGroups[binsPerThread];
    LocalSort localSorts[binsPerThread];
    uint newBinCount = 0;
    uint amtGroups = 0; //sum of the needed work groups for the histogram calculation
    uint amtLocalSorts = 0; //sum of needed local sort groups
    for(int i = 0; i < binsPerThread; ++i){
        localBins[i] = uniformInfo[front].globalHistograms[gl_WorkGroupID.x].keyCount[gl_LocalInvocationID.x * binsPerThread + i];
        uint curBinSize = localBins[i];
        if(gl_LocalInvocationID.x * binsPerThread + i != 0)
            curBinSize = max(0, curBinSize - uniformInfo[front].globalHistograms[gl_WorkGroupID.x].keyCount[gl_LocalInvocationID.x * binsPerThread + i - 1]);
        //check if local sort is sufficient or another radix sort is needed
        if(curBinSize > KPLSB){
            binsSizes[i] = curBinSize;
            binsGroups[i] = (curBinSize + KPB - 1) / KPB;
            amtGroups += binsGroups[i];
            ++newBinCount;
        }
        else if (curBinSize > 0){   //sorting done in local sorts
            binsSizes[i] = 0;
            binsGroups[i] = 0;
            if(amtLocalSorts != 0 && localBins[i] + curStart - localSorts[amtLocalSorts - 1].begin <= KPLSB){ //add to previous localSort
                localSorts[amtLocalSorts - 1].end = curStart + localBins[i];
            }
            else{            //new local sort thread
                localSorts[amtLocalSorts].begin = curStart + localBins[i] - curBinSize;
                localSorts[amtLocalSorts].end = curStart + localBins[i];
                localSorts[amtLocalSorts].front = front; //the sorted bin resides in the back buffer. The front buffer indicates where the keys are lying...
                ++amtLocalSorts;
            }
        }
        else{
            binsSizes[i] = 0;
            binsGroups[i] = 0;
        }
    }
    //subgroup reduction for absolute new bin count + exclusive add to have the correct offsets
    uint bucketHistogramOffset = subgroupExclusiveAdd(newBinCount);
    uint groupsOffset = subgroupExclusiveAdd(amtGroups);
    newBinCount = subgroupAdd(newBinCount);
    amtGroups = subgroupAdd(amtGroups);
    //reserving memroy for new histograms and new subgroups
    uint histIndex = 0;
    uint groupIndex = 0;
    if(gl_LocalInvocationID.x == 0){
        histIndex = atomicAdd(uniformInfo[back].amtOfGlobalHistograms, newBinCount);
        groupIndex = atomicAdd(uniformInfo[back].amtOfBlocks, amtGroups);
    }
    histIndex = subgroupBroadcastFirst(histIndex);
    groupIndex = subgroupBroadcastFirst(groupIndex);
    uint binHistOffset = 0;
    uint blockOffset = groupIndex + groupsOffset;
    for(int i = 0; i < binsPerThread; ++i){
        if(binsSizes[i] != 0){
            uint curHistIndex = histIndex + bucketHistogramOffset + binHistOffset;
            uint curStartOffset = curStart + localBins[i] - binsSizes[i];
            uniformInfo[back].globalHistograms[curHistIndex].startOffset = curStartOffset;
            uniformInfo[back].globalHistograms[curHistIndex].endOffset = curStart + localBins[i];
            //maybe nulling not needed or can be done more efficiently
            for(uint nk = 0; nk < NUMKEYS; ++nk){
                uniformInfo[back].globalHistograms[curHistIndex].keyCount[nk] = 0;
            }
            for(uint g = 0; g < binsGroups[i]; ++g){
                groupInfos.i[blockOffset].globalHistIndex = curHistIndex;
                groupInfos.i[blockOffset].startOffset = curStartOffset + g * KPB;
                // no nulling needed, is set via assignment
                ++blockOffset;
            }
            ++binHistOffset;
        }
    }

    //add local sort to the local sort array(offset is taken by atomic adding to the xLocalSortSize dispatch size)
    uint localSortSum = subgroupAdd(amtLocalSorts);
    uint offset = subgroupExclusiveAdd(amtLocalSorts);
    //reduce local sorts accross subgroups
    for(int i = 0; i < amtLocalSorts; i++){
        sharedLocalSorts[offset + i].begin = localSorts[i].begin;
        sharedLocalSorts[offset + i].end = localSorts[i].end;
        sharedLocalSorts[offset + i].front = localSorts[i].front;
    }
    if(subgroupElect())
        localSortAmt = 0;
    if(localSortSum > 0 && subgroupElect()){    //single thread reduction
        amtLocalSorts = 1;
        for(int i = 1; i < localSortSum; ++i){
            if(sharedLocalSorts[i].end - sharedLocalSorts[amtLocalSorts - 1].begin <= KPLSB){
                sharedLocalSorts[amtLocalSorts - 1].end = sharedLocalSorts[i].end;
            }
            else{
                sharedLocalSorts[amtLocalSorts] = sharedLocalSorts[i];
                amtLocalSorts++;
            }
        }
        localSortAmt = amtLocalSorts;
        localSortOffset = atomicAdd(dispatchInfo.xLocalSortSize, amtLocalSorts);
    }
    for(uint i = gl_LocalInvocationID.x; i < localSortAmt; i += SUBGROUP_SIZE){
        localSortInfo.sorts[localSortOffset + i] = sharedLocalSorts[i];
        //localSortInfo.sorts[localSortOffset + i].begin = gl_WorkGroupID.x;
        //localSortInfo.sorts[localSortOffset + i].end = dispatchInfo.xCtrlSize;
        //localSortInfo.sorts[localSortOffset + i].front = back;
    }
}